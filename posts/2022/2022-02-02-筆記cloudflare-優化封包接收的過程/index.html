<!DOCTYPE html>
<html lang="en"><head>
	
	<meta name="generator" content="Hugo 0.81.0" />
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<meta name="description" content="筆記一些 CloudFlare 有趣的文章，關於如何優化作業系統接收封包的過程"><meta property="og:title" content="筆記：CloudFlare 優化封包接收的過程" />
<meta property="og:description" content="筆記一些 CloudFlare 有趣的文章，關於如何優化作業系統接收封包的過程" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://yuanchieh.page/posts/2022/2022-02-02-%E7%AD%86%E8%A8%98cloudflare-%E5%84%AA%E5%8C%96%E5%B0%81%E5%8C%85%E6%8E%A5%E6%94%B6%E7%9A%84%E9%81%8E%E7%A8%8B/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-02-02T01:21:40&#43;00:00" />
<meta property="article:modified_time" content="2022-02-02T01:21:40&#43;00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="筆記：CloudFlare 優化封包接收的過程"/>
<meta name="twitter:description" content="筆記一些 CloudFlare 有趣的文章，關於如何優化作業系統接收封包的過程"/>

	<link rel="stylesheet" type="text/css" media="screen" href="/css/normalize.css" />
	<link rel="stylesheet" type="text/css" media="screen" href="/css/main.css" />
	<link rel="stylesheet" type="text/css" media="screen" href="/css/all.css" />
	<title>筆記：CloudFlare 優化封包接收的過程 | Yuanchieh&#39;s Blog</title><meta name="google-site-verification" content="Zmi0DCwmB_paCE45T_J552m4NDDpaMH6dqZLMYgM7Js" />
</head>
<body><header>
	
	<div id="avatar">
		<a href="https://yuanchieh.page">
		  <img src="https://avatars0.githubusercontent.com/u/6858460?s=460&amp;v=4" alt="Yuanchieh&#39;s Blog">
		</a>
	</div>
	
	<div id="titletext"><h2 id="title"><a href="https://yuanchieh.page">Yuanchieh&#39;s Blog</a></h2></div>
	<div id="title-description"><p id="subtitle">生命是長期而持續的累積</p><div id="social">
			<nav>
				<ul>
					<li><a href="https://github.com/sj82516"><i title="Github" class="icons fab fa-github"></i></a></li><li><a href="mailto:sj82516@gmail.com"><i title="Email" class="icons fas fa-envelope"></i></a></li>
					<li><a href="/index.xml"><i title="RSS" class="icons fas fa-rss"></i></a></li></ul>
			</nav>
		</div>
	</div>
	
	<div id="mainmenu">
		<nav>
			<ul>
				
				<li><a href="/posts">All Posts</a></li>
				
				<li><a href="/about">About</a></li>
				
				<li><a href="/categories">Categories</a></li>
				
			</ul>
		</nav>
	</div>
	
</header>
<main><div class="post">
	
	<div class="author">
	
	</div>
	<div class="post-header">
	
		<div class="meta">
			
			<div class="date">
				<span class="day">02</span>
				<span class="rest">Feb 2022</span>
			</div>
			
		</div>
		
		<div class="matter">
			<h1 class="title">筆記：CloudFlare 優化封包接收的過程</h1>
		</div>
	</div>
	<div class="markdown">
		<p>最近想了解一下作業系統處理 I/O 的過程，翻閱到 CloudFlare 一系列關於作業系統與 Network Packet 處理的實驗與文章，蠻有趣的稍微整理了一下</p>
<h2 id="關於-linux-封包處理">關於 Linux 封包處理</h2>
<p>主要在量測 Linux 極限可以處理多少 packet per second / 以及如何持續優化</p>
<h3 id="linux-單機處理-1m-udp-packet-per-second">Linux 單機處理 1M udp packet per second</h3>
<p>文章出自：<a href="https://blog.cloudflare.com/how-to-receive-a-million-packets/">How to receive a million packets per second</a>，在硬體規格</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fallback" data-lang="fallback">1. six core 2GHz Xeon processors.   
2. With hyperthreading (HT) enabled that counts to 24 processors on each box
3. multi-queue 10G network card by Solarflare
4. with 11 receive queues configured.
</code></pre></div><p>可以撐到每秒接收一百萬個 udp 小封包(32 bytes)，發送跟接收端在同個區網的不同機器上</p>
<p>中間的嘗試迭代過程很有趣</p>
<h4 id="1-裸測">1. 裸測</h4>
<p>僅能到 0.19~0.35 M，發現 kernel 可能隨機分配 process 到不同 CPU 上，透過 <code>taskset</code> 指定 CPU 運行，穩定在 0.35 M 上不去，發現只有單一 CPU 忙碌到 100%</p>
<h4 id="2-numa-架構優化">2. NUMA 架構優化</h4>
<p>多核心處理器為了避免多核心在存取 Memory 的貧頸，會採用 NUMA 架構，讓一至多個 CPU core 共用一塊 memory 組成一個 NUMA node；
<img src="https://www.motioncontroltips.com/wp-content/uploads/2018/04/NUMA-Architecture.png" alt=""></p>
<ul>
<li>圖片出處 <a href="https://hackmd.io/@lkmem/SkWYoA-TU">@lkmem/SkWYoA-TU</a></li>
</ul>
<p>NUMA 好處是如果 CPU 都只存取 local memory 效率會非常好，如果要存取到不同區域的 memory 或是任務在不同 NUMA node CPU 中 context switch 則效率會很差，作者進行以下比較：</p>
<ol>
<li>如果 RX queue 和應用程式是在同一個 NUMA node 的不同 core 上，效能是最好的</li>
<li>如果是在不同的 NUMA node 上，則效能差且不穩定</li>
<li>如果是在同個超執行緒(HT)上，則效能只剩一半</li>
</ol>
<h4 id="3-增加-nic-的-rx-queue-數量">3. 增加 NIC 的 RX queue 數量</h4>
<p>過往 NIC 只有一個 RX queue 用來存放接收到的封包，而<code>一個 RX queue 只能由一個 CPU 讀取</code>限制了多核心機器的效能，所以 NIC 目前都會有多條 RX queue</p>
<p>但是有了多條 queue，NIC 需要把同一個 connection 的多個 packet 送到同一個 queue 中讓同一個 CPU 處理，否則會遇到 packet order 亂掉的問題，此時可以透過 hashing 解決，hash function 為 {ip, port}</p>
<p>後來因為實驗的 NIC 無法調整 hash 機制，所以改用增加 IP 的方式，但目的也是要把 <code>packet 分散到多個 RX queue 上</code>，此舉增加到 0.47M</p>
<h4 id="4-加開-recv-thread">4. 加開 recv thread</h4>
<p>原本是用 single thread 接收，嘗試開啟 multi thread 但反而性能下降，原因是多個 thread 共用同一個 file descriptor 需要額外 lock</p>
<p>但好在 Linux 在 3.9 後加入了 <code>SO_REUSEPORT</code> 可以讓多個socket descriptor 綁定到同一個 port 上，所以讓多個 thread 可以分擔讀取的工作，但作者同樣觀察到 SO_REUSEPORT 這一層同樣有分散不平均的問題，只有幾個 CPU 特別忙碌</p>
<blockquote>
<p>One of the features merged in the 3.9 development cycle was TCP and UDP support for the SO_REUSEPORT socket option; that support was implemented in a series of patches by Tom Herbert. The new socket option allows <code>multiple sockets on the same host to bind to the same port</code>, and is intended to improve the performance of <code>multithreaded network server</code> &hellip;..</p>
<p>&hellip;. Incoming connections and datagrams are distributed to the server sockets using a <code>hash based on the 4-tuple of the connection</code>—that is, the peer IP address and port plus the local IP address and port.</p>
</blockquote>
<h4 id="小結">小結</h4>
<p>單機 Linux 優化後可以做到每秒接收 1M UDP packet</p>
<ol>
<li>記得將 packet 分散到 RX queue</li>
<li>接收端應用程式要打開 SO_REUSEPORT 並透過 multi thread 讀取</li>
<li>要有多的 CPU 負責從 RX queue 讀取</li>
<li>如果是 NUMA 架構，RX queue / 應用程式的 CPU 要在同一個 Node 性能比較好</li>
</ol>
<h3 id="為什麼我們需要-linux-kernel-處理-tcp-stack">為什麼我們需要 Linux kernel 處理 TCP stack?</h3>
<p>文章出自：<a href="https://blog.cloudflare.com/why-we-use-the-linux-kernels-tcp-stack/">Why we use the Linux kernel&rsquo;s TCP stack</a></p>
<h4 id="為什麼我們需要-os">為什麼我們需要 OS</h4>
<p>試想一個問題，如果我們一台 server 上只跑一個專用的應用程式，那為什麼我們需要先額外安裝一個上千萬行的 OS 去代理執行我們的應用程式？</p>
<p>OS 主要提供幾個好處：</p>
<ol>
<li><strong>抽象化硬體、提供統一介面：</strong><br>
OS 會抽象化底層的硬體，讓應用程式專注於開發而不用管實際執行的硬體為何，帶來更好的移植性</li>
<li><strong>資源管理：</strong><br>
OS 可以透過排程讓一個硬體於多個應用程式間切換使用，而不會有一個應用程式霸佔整個硬體資源，例如網卡可同時處理 server request 也可用於 ssl sesson</li>
</ol>
<h4 id="為什麼需要-userspace-tcp-stack">為什麼需要 userspace TCP stack</h4>
<p>如果透過 OS 則讀寫硬體資源時需要涉及 user space / kernel space 資料的複製與 context switch，這會帶來額外的 latency / CPU performance 影響</p>
<p>例如 Google、Cloudflare 這種網路流量非常大的公司，就會有動機去客製化 TCP stack，所謂的 userspace TCP stack 就是 <code>繞過(bypass) OS Kernel</code>直接存取網卡，這張網卡就專門被單一的應用程式所使用，OS 的監控工具 (iptable/netstat) 等都無法使用，其他應用程式也都不能</p>
<p>Cloudflare 有提到透過 Linux iptable 大概可以處理<code>每秒一百萬的 packets</code>，而 Cloudflare 在遭遇攻擊時流量會在 <code>單台 server 每秒三百萬的 packets</code>，這也是他們需要繞過 kernel 自行處理的動機</p>
<p>但目前沒有穩定的 open-source userspace TCP stack 可以使用，要自己維護開發，除了執行外周邊的 debug、monitor 工具都要自己想辦法<code>成本很高</code>，所以 Cloudflare 採取<code>半繞過</code>的方式，只有 &ldquo;RX queue&rdquo; 會繞過 kernel，這帶來的好處是享有一定的效能提升且其餘 packets 處理能仰賴既有的工具如 iptables/netstat</p>
<h3 id="優化-latency">優化 latency</h3>
<p>文章出自：<a href="https://blog.cloudflare.com/how-to-achieve-low-latency/">How to achieve low latency with 10Gbps Ethernet</a><br>
前面優化了 throughput，現在要來看 latency 如何被優化，量測的基準線從平均 47.5 us 開始</p>
<h4 id="1-提高-read-的頻次">1. 提高 read 的頻次</h4>
<p>Linux 3.11 開始增加了 <code>SO_BUSY_POLL</code> 的 socket 選項，在指定的時間內 kernel 就會去讀取 packet 提升讀取的頻次，透過增加 CPU 使用率降低 latency 7 us</p>
<p>同樣的道理可以套用在 user space ，透過 non-blocking read <code>fd.recvmsg(MSG_DONTWAIT)</code> 再往下降低 4 us</p>
<h4 id="2-pin-process">2. pin process</h4>
<p>透過指定 process 在特定的 CPU 上執行減少 context switch，降低 1 us；<br>
但如同先前提到，預設一個 RX queue 只能有單一 CPU 讀取，如果剛好應用程式也在最忙碌的 CPU 上，則延遲反而會增加 2 us</p>
<h4 id="3-將-rx-queue-綁定到指定-cpu">3. 將 RX queue 綁定到指定 CPU</h4>
<p>為了避免上面問題，可以透過 Indirection Table 調整 RX queue 如何分配到 CPU 讀取，作者限定只有在同一個 NUMA node 的 CPU 才能讀取 RX queue</p>
<p>同樣的做法也可以用 Flow steering 實作，指定特定的 flow 到特定的 RX queue 上面，額外用途是確保在高流量情況下指定的封包還能被特定的 CPU 處理，如 SSH/BGP，也可以用來防堵 DDoS 攻擊</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fallback" data-lang="fallback">client$ sudo ethtool -N eth2 flow-type udp4 dst-ip 192.168.254.1 dst-port 65500 action 1
</code></pre></div><p>在傳送端也可以指定類似的事情，調整 TX queue 透過哪個 CPU 發送的機制</p>
<h4 id="小結-1">小結</h4>
<p>再通過一些優化，從原本 47 us 降到 26 us；如果是透過 bypass kernel 則近一步下降到 <code>17 us</code></p>

	</div>
	
	
	
	
	
	
	
	<div class="tags">
		<div class="taxosfloating_left">
			<p>Categories</p>
		</div>
		<div class="termsfloating_right">
			<p>
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			<a href="/categories/%e7%b6%b2%e8%b7%af%e8%88%87%e5%8d%94%e5%ae%9a/"> 網路與協定 </a>
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			</p>
		</div>
		<div class="clearit"></div>
		
		
		
		
	
		
	
		
		
	</div>
<div id="disqus_thread"></div>
<script type="text/javascript">
	(function() {
	    
	    
	    if (window.location.hostname == "localhost")
	        return;
	    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
	    var disqus_shortname = 'yuanchieh';
	    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
	    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
	})();
</script>
<noscript>Please enable JavaScript to load the comments.</noscript>


</div>

  </main>


<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-82837682-4', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>




</body>
</html>
